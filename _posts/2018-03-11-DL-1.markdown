---
layout:     post
title:      "Deep Learning笔记1"
subtitle:   "Logistic Regression 逻辑回归"
date:       2018-03-09
author:     "mingjie"
header-img: "img/post-bg-DL-1.jpg"
tags:
    - learning

---

#### Binary Classification

Binary Classification指的是输入一堆东西，最后给出一个只有两个结果（0或者1）的输出的分类过程。一个典型而符合惯例的例子是判断某张图片上有没有猫，也就是将图片的红绿蓝像素值矩阵作为输入，是猫(1)以及不是猫(0)作为输出。

在这里我们不考虑这么多细节，直接将红绿蓝的像素排成一列，作为输入$$ x $$。$$ x $$的大小就是总的像素数$$ n_x $$或者$$ n $$。

**如果按照位置来排呢？**

![](/img/in-post/post-DL-1/1-1.png)

需要在开始之前明确一些记号。

我们在寻找解的时候的输入为$$ (x, y) $$，当然就是$$ (图片, 是否猫) $$的组合了。其中$$ x $$是一个$$ n_x $$行的**列向量**，即$$ x \in \mathbb{R}^{n_x} $$；而$$ y \in \{0, 1\} $$。当然这只是一个样本，而我们一般会有很多的**训练样本**(training set)：$$ \{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)}) \} $$。$$ m $$即为训练样本的数量；当我们需要明确的时候我们会把训练样本数和**检验样本**(test set)数写成$$ m_\mathrm{train}, m_\mathrm{test} $$。

这么写当然比较麻烦，所以我们有矩阵写法。首先将所有的$$ x $$排成一行，就成了一个$$ n_x $$行$$ m $$列的矩阵$$ \boldsymbol{X} $$。数学语言表达就是$$ \boldsymbol{X} \in \mathbb{R}^{n_x\times m} $$，python表达就是`X.shape = (nx, m)`。把$$ \boldsymbol{X} $$转置也不是不行，不过会比较麻烦，就不用了。

相应的所有的label也这么排，就成了一个一行$$ m $$列的“矩阵”$$ \boldsymbol{Y} = [y^{(1)}, y^{(2)}, ..., y^{(m)}] $$；或者说$$ \boldsymbol{Y} \in \mathbb{R}^{1\times m} $$，`Y.shape = (1, m)`。

更多的记号可参阅课程给的pdf。

![](/img/in-post/post-DL-1/1-2.png)

#### 逻辑回归

实际上逻辑回归是在二项分布之下的一个概念，是实现二项分布的一个方法。我们需要的是在上面的定义下，构造一个盒子，接收输入$$ x $$, 并输出在输入$$ \mathbf{x} $$的情况下二项分布结果为1的概率$$ \hat{y} = P(y=1 \lvert \boldsymbol{x}) $$。然后我们有相应的参数$$ \boldsymbol{w} \in \mathbb{R}^{n_x}, b \in \mathbb{R} $$。

构造输出$$ \hat{y} $$的第一反应应该是

$$ \hat{y} = \boldsymbol{w}^\mathrm{T} \boldsymbol{x} + b $$

这其实就是普通的线性回归。但是我们要求$$ \hat{y} $$是一个概率，取值在0到1之间，而线性回归不仅不能保证结果小于1还会出现负的结果，当然是不对的。

那怎么办？很简单，给线性回归的结果套一个值域在0到1之间的函数不就行了呗。这种函数一般都跟$$ e $$指数有关，所以我们有sigmod函数：

$$ \sigma(z) = \frac{1}{1+e^{-z}} $$

图像请看截图的左下角。

有了sigmod函数之后，只要令

$$ \hat{y} = \sigma(\boldsymbol{w}^\mathrm{T} \boldsymbol{x} + b) $$

就行了。加上了sigmod函数之后的回归就叫逻辑回归。有的方法把$$ b $$也放进$$ \boldsymbol{w} $$里面，不过这里不这么做。

![](/img/in-post/post-DL-1/1-3.png)

#### 代价/损失函数

代价/损失函数就是用来训练上面提到的$$ \boldsymbol{w}, b $$的。这个函数越小，模型的效果越好（对于训练样本来说）。这里定义的损失函数只是对于一个训练样本来说的，而代价函数是对于整个训练样本集在某一堆参数下的评价。

学过线性拟合的同学们马上又该反应过来，一个典型的代价函数就是$$ L(\hat{y}, y) = (\hat{y} - y)^2 $$。但是这个函数的效果并不好，所以我们实际上用的是这个：

$$ L(\hat{y}, y) = -[y \ln{\hat{y}} + (1-y) \ln{(1-\hat{y})}] $$

大致地说为什么代价函数的样子是这样的原因是，差值的平方会导致函数非凸，但是上面的定义加上sigmod函数会使得$$ L $$是个凸函数，没有局部极值。

对应的代价函数我们选择这个：

$$
\begin{align}
J(\boldsymbol{w}, b) &= \frac{1}{m} \sum_{i=1}^{m} L(\hat{y}^{(i)}, y^{(i)}) \\
 &= - \frac{1}{m} \sum_{i=1}^{m} [y \ln{\hat{y}} + (1-y) \ln{(1-\hat{y})}]
\end{align}
$$

只要找到最小化$$ \boldsymbol{w}, b $$的$$ J $$，就行了。实际上逻辑回归就是一个小型的（可能还是最简单的）神经网络。

![](/img/in-post/post-DL-1/1-4.png)

#### 如何训练参数？

最小化$$ \boldsymbol{w}, b $$就是在$$ J(\boldsymbol{w}, b) $$这个表面上（简单来说）迭代地找到这个函数的最小值以及对应的参数。又该反应过来我们需要用到导数来指导我们向哪里走了。具体来说，是计算这个：

$$
\begin{align}
\boldsymbol{w} &:= \boldsymbol{w} - \alpha \frac{\partial J}{\partial \boldsymbol{w}}\\
b &:= b - \alpha \frac{\partial J}{\partial b}
\end{align}
$$

$$ \alpha $$被称为学习速率，是人选的；而后面的俩偏导数是需要计算的，在python中会被命名为`dw, db`。

![](/img/in-post/post-DL-1/1-5.png)

#### 正反向传递

具体请看截图。正向传递的意思是从具体参数计算代价函数值的过程（蓝色、从左到右），而反向传递指的是计算导数的过程（红色、从右到左）。

![](/img/in-post/post-DL-1/1-6.png)

#### 导数的具体计算

我们举一个简单的例子：

$$ \begin{align}
z &= w_1 x_1 + w_2 x_2 + b \\
\hat{y} &= \sigma(z) \\
L(\hat{y}, y) &= -[y \ln{\hat{y}} + (1-y) \ln{(1-\hat{y})}] \\
\end{align}
$$

自然地

$$
\begin{align}
\frac{dL}{d\hat{y}} &= -\frac{y}{\hat{y}} + \frac{1-y}{1-\hat{y}} \\
\frac{d\hat{y}}{dz} &= \hat{y} (1-\hat{y}) = \sigma (1-\sigma) = \hat{y} - y \\
\frac{dz}{dw_1} &= x_1 \frac{d\hat{y}}{dz} \\
\frac{dz}{dw_2} &= x_2 \frac{d\hat{y}}{dz} \\
\frac{dz}{db} &= \frac{d\hat{y}}{dz} \\
\end{align}
$$

在代码中我们基本上会用导数的分母作为变量名字。

![](/img/in-post/post-DL-1/1-7.png)

之后我们需要算的就是代价函数了。其实也没什么，就拿一个变量做代表吧：

$$ \frac{\partial J}{\partial w_1} = \frac{1}{m} \sum_{i=1}^{m} \frac{L^{(i)}}{\partial w_1} $$

具体的计算过程就是一个正向+反向传递的过程：

```py
# 设置初值
J = 0; dw1 = 0; ...; db = 0
for i in np.arange(1,m+1):
  # Forward propagation
  z[i] = w.T * x[i] + b
  a[i] = sigmod(z[i])
  J += L(a[i], y[i])
  # Backward propagation
  dz[i] = a[i] - y[i]
  dw1 += x1[i]dz[i]
  ...
  db += dz[i]
J /= m; dw1 /= m; ...; db /= m
w1 = w1 - alpha * dw1
...
b = b - alpha * db
```

然后继续迭代。所以“正常”情况下要用两个循环嵌套，显然是不现实的，所以要用到python的向量来做。

#### 向量化

当`w`是一个列向量的时候，比如要算`z[i] = w.T * x[i] + b`的时候，可以这么做：`np.dot(w,x)+b`。高级一点的话直接和矩阵相乘：`np.dot(w,X)+b`。
`dz[i] = a[i] - y[i]`可以转为`dZ = A - Y`；`dw1 += x1[i]dz[i]` -> `dW = 1/m * X*dZ.T`；`db += dz[i]` -> `1/m * np.sum(dZ)`。还要注意的一点是在`reshape`的时候最好把行列都写全。
