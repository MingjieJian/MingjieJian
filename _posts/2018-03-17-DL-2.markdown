---
layout:     post
title:      "Deep Learning笔记2"
subtitle:   "Shallow Neural Network 神经网络"
date:       2018-03-17
author:     "mingjie"
header-img: "img/post-bg-DL-1.jpg"
tags:
    - learning

---

[1],[2]代表神经网络的层数；(i)代表某一个训练样本（都是上标）。下标不带任何括号，表示某一层中的第几个节点。当有两层的时候，刚开始的输入是$$ x $$，但是之后的输入会变成了$$ a^{[i]} $$这样。

#### 神经网络的命名

输入层、隐层、输出层。$$ a^{[0]} = x $$。在数层数的时候，不算输入层。每一层都有一套$$ w, b $$配套。



![](/img/in-post/post-DL-2/2-1.png)

每一个节点都有两步：线性回归以及丢进sigmod函数里面。所以都有两条公式：

$$
\begin{align}
  z_1^{[1]} &= w_1^{[1]T}x + b_1^{[1]}, a_1^{[1]} = \sigma(z_1^{[1]}) \\
  z_2^{[1]} &= w_2^{[1]T}x + b_2^{[1]}, a_2^{[1]} = \sigma(z_2^{[1]}) \\
  z_3^{[1]} &= w_3^{[1]T}x + b_3^{[1]}, a_3^{[1]} = \sigma(z_3^{[1]}) \\
  z_4^{[1]} &= w_4^{[1]T}x + b_4^{[1]}, a_4^{[1]} = \sigma(z_4^{[1]}) \\
  \vdots
\end{align}
$$

能看到这些式子排得很整齐，那何不矩阵化了呢？

$$ z^{[1]} =
  \begin{bmatrix}
    z_1^{[1]} \\ z_2^{[1]} \\ z_3^{[1]} \\ z_4^{[1]}
  \end{bmatrix}
  =
  \begin{bmatrix}
    \cdots w_1^{[1]T} \cdots \\ \cdots w_2^{[1]T} \cdots \\ \cdots w_3^{[1]T} \cdots \\ \cdots w_4^{[1]T} \cdots
  \end{bmatrix}
  \begin{bmatrix}
    x_1 \\ x_2 \\ x_3
  \end{bmatrix}
  +
  \begin{bmatrix}
    b_1^{[1]} \\ b_2^{[1]} \\ b_3^{[1]} \\ b_4^{[1]}
  \end{bmatrix}
  = W^{[1]} x + b^{[1]}
$$

$$ W^{[i]} $$是一个$$ [i] $$（本层节点数）行$$ [i-1] $$（上层节点数）列的矩阵，也就是负责将上层节点的结果转成本层节点的结果。$$ b^{[i]} $$自然是$$ [i] $$行1列的向量了。最后再将结果各自丢进sigmod函数里面，不说了。

如果写出个通式的话也行：

$$
\begin{align}
   z^{[i]} &=  W^{[i]} a^{[i-1]} + b^{[i]} \\
   a^{[i]} &= \sigma(z^{[i]})
\end{align}
$$

不过注意$$ a^{[0]} $$就是输入值，没有放入sigmod函数里面。同时矩阵$$ W^{[i]} $$是一个“行矩阵”，每一行是每一个节点的系数的转置$$ [\cdots w_i^{[1]T} \cdots] $$。

![](/img/in-post/post-DL-2/2-2.png)

![](/img/in-post/post-DL-2/2-3.png)

#### 对于整个训练样本

当然可以继续矩阵化啦。把整个训练样本的列向量$$ x,b $$排成一行（这也是惟一的排法了），结果$$ z $$也这么做，然后大写，就变成了：

$$
\begin{align}
Z^{[1]} &=
  \begin{bmatrix}
    \vdots & \vdots &  & \vdots \\
    z^{[1](1)} & z^{[1](2)} & \cdots & z^{[1](m)} \\
    \vdots & \vdots &  & \vdots \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    \cdots w_1^{[1]T} \cdots \\ \cdots w_2^{[1]T} \cdots \\ \vdots \\ \cdots w_{[i]}^{[1]T} \cdots
  \end{bmatrix}
  \begin{bmatrix}
    \vdots & \vdots &  & \vdots \\
    x^{(1)} & x^{(2)} & \cdots & x^{(m)} \\
    \vdots & \vdots &  & \vdots \\
  \end{bmatrix}
  +
  \begin{bmatrix}
    \vdots & \vdots &  & \vdots \\
    b^{[1]} & b^{[1]} & \cdots & b^{[1]} \\
    \vdots & \vdots &  & \vdots \\
  \end{bmatrix} \\
  &= W^{[1]} X + B^{[1]}
\end{align}  
$$

或者对于某一层来说

$$
\begin{align}
Z^{[i]} &=
  \begin{bmatrix}
    \vdots & \vdots &  & \vdots \\
    z^{[i](1)} & z^{[i](2)} & \cdots & z^{[i](m)} \\
    \vdots & \vdots &  & \vdots \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    \cdots w_1^{[i]T} \cdots \\ \cdots w_2^{[i]T} \cdots \\ \vdots \\ \cdots w_{[i]}^{[i]T} \cdots
  \end{bmatrix}
  \begin{bmatrix}
    \vdots & \vdots &  & \vdots \\
    a^{[i](1)} & a^{[i](2)} & \cdots & a^{[i](m)} \\
    \vdots & \vdots &  & \vdots \\
  \end{bmatrix}
  +
  \begin{bmatrix}
    \vdots & \vdots &  & \vdots \\
    b^{[i]} & b^{[i]} & \cdots & b^{[i]} \\
    \vdots & \vdots &  & \vdots \\
  \end{bmatrix} \\
  &= W^{[i]} X + B^{[i]}
\end{align}  
$$

总结一下，$$ Z^{[i]} $$是一个$$ [i] $$行$$ m $$列的矩阵，$$ W^{[i]} $$是一个$$ [i] $$行$$ [i-1] $$列的矩阵，$$ A^{[i]} $$是一个$$ [i] $$行$$ m $$列的矩阵，$$ B^{[i]} $$也是一个$$ [i] $$行$$ m $$列的矩阵，但是每一列都是一样的。

通式：

$$
\begin{align}
   Z^{[i]} &=  W^{[i]} A^{[i-1]} + B^{[i]} \\
   A^{[i]} &= \sigma(Z^{[i]})
\end{align}
$$

![](/img/in-post/post-DL-2/2-4.png)

![](/img/in-post/post-DL-2/2-5.png)

![](/img/in-post/post-DL-2/2-6.png)

#### 不同的activation function

sigmod函数的问题是它恒为正，所以算出来的结果不会关于0对称。所以隐藏层一般用$$ \tanh(z) $$而不是sigmod，但是输出层还是用回sigmod。

但是$$ \tanh(z) $$也还是有问题：当$$ z $$很大的时候，斜率很小，所以会降低学习速率；所以也可以用RelU函数（折线），或者将RelU小于0的折线再改小一点。即使斜率为0实际上也还是可以的。

![](/img/in-post/post-DL-2/2-7.png)

#### 为什么activation function不是线性的？

也就是说为什么要给它套上一个奇怪的函数？

线性的AF意味着从头到脚都是线性的，也就没有意义（人又不是线性思考的233）。例外是在回归里面，当输出不在0到1之间的时候，最后一层可以用线性的。但是其他层还是要用非线性的。

![](/img/in-post/post-DL-2/2-8.png)

#### AF的导数

求就是了，又不是不会。按照链式反应（chain rule啦）来求就行了。按照之前的结论其实$$ \frac{dL}{da^{[2]}} $$不用求，有$$ \frac{dL}{dz^{[2]}} = a^{[2]} - y$$。

完全向量化之后出现的$$ \frac{1}{m} $$以及求和是因为之前对某一个训练样本来说，只能计算损失函数；但是对于整个训练样本集来说，我们计算的是代价函数，而$$ J = \frac{1}{m}\sum L $$，所以多了点东西。

![](/img/in-post/post-DL-2/2-9.png)

![](/img/in-post/post-DL-2/2-10.png)

#### 随机初值

参数全为0的话，所有的节点都是一样（对称）的，就没用了。所以要随机取值。一般会将$$ w $$取为很小的高斯随机数，但是$$ b $$没所谓。很小的高斯随机数是为了取到比较大的导数值，防止落到导数很靠近0的地方。

![](/img/in-post/post-DL-2/2-11.png)
