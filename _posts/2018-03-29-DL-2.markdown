---
layout:     post
title:      "Deep Learning笔记2"
subtitle:   "调参"
date:       2018-03-14
author:     "mingjie"
header-img: "img/post-bg-DL-1.jpg"
tags:
    - learning

---

#### 数据集

以前是分成两个的，训练和测试。现在分成3个，训练、发展和测试集。用发展去测试训练，最后再用测试去测试表现；甚至没有测试集都行。这里的区别是发展集是用来overfitting的，测试集不是。

要注意不同数据集的数据要同质，质量相同

![](/img/in-post/post-DL-2/1-1.png)

#### bias和variance

训练集的错误率比与预期的高：高bias；发展集的错误率比训练集的高：高variance

![](/img/in-post/post-DL-2/1-2.png)

![](/img/in-post/post-DL-2/1-3.png)

#### 正则化

在代价函数后面加上$$ w $$的2或者1范数

通过强制减小$$ w $$简化神经网络，减小权重。

![](/img/in-post/post-DL-2/1-4.png)

![](/img/in-post/post-DL-2/1-5.png)

#### Dropout正则化

随机删掉一些节点来做一次正反向传递，然后不断重复。要注意的是删掉之后要把整个节点的值除以一个删掉的阈值概率，把取值回覆到正常水平。测试集不要用。Dropoyt的意思是不让某个节点依赖于某几个前面一层的节点，所以也是减小权重的意思。不同的层可以有不同的dropout节点数。输入层一般不dropout（或者是很小的duopout数）。最后在看J减小的时候要小心一点，必要的时候关掉dropout

![](/img/in-post/post-DL-2/1-6.png)

#### 奇技淫巧

增加训练样本：把图像翻过来/旋转/放大，或者几种加起来。不过信息量其实还是一样的。标准是人脑还是会把它当成同一样东西。

提早停止：看发展集的误差，以它的最小值为基准。思路是$$ w $$会随着迭代而不断增大，导致太大以至于我们不想看到。提早停止可以防止这个。

正交化：将减小损失函数和防止overfit分开。提早停止违反了这个原则。


L2正则化比较简单，不过要试各种的$$ \lambda $$。

### 建立最优化问题

#### 归一化训练集

看图，把输入数字移到0左右，将标准差scale到1。注意训练集和测试集的移动和scale需要完全一样。

这么做的理由：将代价函数搞得好看一点，简单一点，学习速率也能快一点。

![](/img/in-post/post-DL-2/1-7.png)

![](/img/in-post/post-DL-2/1-8.png)

#### 奇怪的导数值

在训练很深的神经网络的时候，有的时候导数值可能会很大或者很小，比如矩阵遇到了指数增长/减小什么的。

一个治标的办法是选择初始的权重。$$ z = w_1x_1 + w_2x_2 + \cdots + w_nx_n $$。$$ n $$越大，$$ z $$就越大，所以我们控制$$ w $$的标准差为$$ \sqrt{\frac{2}{n^{[l-1]}}} $$。这里的2是Relu函数带来的。当然也可以当它是一个超参数来试。

![](/img/in-post/post-DL-2/1-9.png)


#### 导数值检查

往左和往右挪，计算$$ \frac{y}{x} $$。愣算就行。

具体：将所有的参数竖着排成一个巨长的向量，包括正向的和反向的。然后愣算其中某个参数的导数，看两个向量之间的二范数比各自的二范数的大小。比值是拿来看相对大小的，防止它们本来就很小或者大。

检查只能用在debug上。如果出错了，看具体的值哪个不正常。如果有的话，记得加上正则化项。检查不能用在dropout上，要用的话先关掉。

![](/img/in-post/post-DL-2/1-10.png)
