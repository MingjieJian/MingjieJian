<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Write everything, think everything.">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/myicon.ico">

    <title>Deep Learning笔记1 - Mingjie的博客 | Mingjie's Blog</title>

    <link rel="canonical" href="http://localhost:4000/2018/03/14/DL-1/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/hux-blog.min.css">

    <!-- Pygments Github CSS -->
    <link rel="stylesheet" href="/css/syntax.css">

    <!-- Custom Fonts -->
    <!-- <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="https://cdn.staticfile.org/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Mingjie's Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>
                    
                    <li>
                        <a href="/about/">About</a>
                    </li>
                    
                    <li>
                        <a href="/tags/">Tags</a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Image to hack wechat -->
<!-- <img src="/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="/img/post-bg-DL-1.jpg" width="0" height="0"> -->

<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        position: relative;
        background-image: url('/img/post-bg-DL-1.jpg')
    }

    
</style>
<header class="intro-header" >
    <div class="header-mask"></div>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/tags/#learning" title="learning">learning</a>
                        
                    </div>
                    <h1>Deep Learning笔记1</h1>
                    
                    
                    <h2 class="subheading">逻辑回归与神经网络</h2>
                    
                    <span class="meta">Posted by mingjie on March 14, 2018</span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

				<h3 id="逻辑回归">逻辑回归</h3>

<h4 id="binary-classification">Binary Classification</h4>

<p>Binary Classification指的是输入一堆东西，最后给出一个只有两个结果（0或者1）的输出的分类过程。一个典型而符合惯例的例子是判断某张图片上有没有猫，也就是将图片的红绿蓝像素值矩阵作为输入，是猫(1)以及不是猫(0)作为输出。</p>

<p>在这里我们不考虑这么多细节，直接将红绿蓝的像素排成一列，作为输入<script type="math/tex">x</script>。<script type="math/tex">x</script>的大小就是总的像素数<script type="math/tex">n_x</script>或者<script type="math/tex">n</script>。</p>

<p><strong>如果按照位置来排呢？</strong></p>

<p><img src="/img/in-post/post-DL-1/1-1.png" alt="" /></p>

<p>需要在开始之前明确一些记号。</p>

<p>我们在寻找解的时候的输入为<script type="math/tex">(x, y)</script>，当然就是<script type="math/tex">(图片, 是否猫)</script>的组合了。其中<script type="math/tex">x</script>是一个<script type="math/tex">n_x</script>行的<strong>列向量</strong>，即<script type="math/tex">x \in \mathbb{R}^{n_x}</script>；而<script type="math/tex">y \in \{0, 1\}</script>。当然这只是一个样本，而我们一般会有很多的<strong>训练样本</strong>(training set)：<script type="math/tex">\{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)}) \}</script>。<script type="math/tex">m</script>即为训练样本的数量；当我们需要明确的时候我们会把训练样本数和<strong>检验样本</strong>(test set)数写成<script type="math/tex">m_\mathrm{train}, m_\mathrm{test}</script>。</p>

<p>这么写当然比较麻烦，所以我们有矩阵写法。首先将所有的<script type="math/tex">x</script>排成一行，就成了一个<script type="math/tex">n_x</script>行<script type="math/tex">m</script>列的矩阵<script type="math/tex">\boldsymbol{X}</script>。数学语言表达就是<script type="math/tex">\boldsymbol{X} \in \mathbb{R}^{n_x\times m}</script>，python表达就是<code class="highlighter-rouge">X.shape = (nx, m)</code>。把<script type="math/tex">\boldsymbol{X}</script>转置也不是不行，不过会比较麻烦，就不用了。</p>

<p>相应的所有的label也这么排，就成了一个一行<script type="math/tex">m</script>列的“矩阵”<script type="math/tex">\boldsymbol{Y} = [y^{(1)}, y^{(2)}, ..., y^{(m)}]</script>；或者说<script type="math/tex">\boldsymbol{Y} \in \mathbb{R}^{1\times m}</script>，<code class="highlighter-rouge">Y.shape = (1, m)</code>。</p>

<p>更多的记号可参阅课程给的pdf。</p>

<p><img src="/img/in-post/post-DL-1/1-2.png" alt="" /></p>

<h4 id="逻辑回归-1">逻辑回归</h4>

<p>实际上逻辑回归是在二项分布之下的一个概念，是实现二项分布的一个方法。我们需要的是在上面的定义下，构造一个盒子，接收输入<script type="math/tex">x</script>, 并输出在输入<script type="math/tex">\mathbf{x}</script>的情况下二项分布结果为1的概率<script type="math/tex">\hat{y} = P(y=1 \lvert \boldsymbol{x})</script>。然后我们有相应的参数<script type="math/tex">\boldsymbol{w} \in \mathbb{R}^{n_x}, b \in \mathbb{R}</script>。</p>

<p>构造输出<script type="math/tex">\hat{y}</script>的第一反应应该是</p>

<script type="math/tex; mode=display">\hat{y} = \boldsymbol{w}^\mathrm{T} \boldsymbol{x} + b</script>

<p>这其实就是普通的线性回归。但是我们要求<script type="math/tex">\hat{y}</script>是一个概率，取值在0到1之间，而线性回归不仅不能保证结果小于1还会出现负的结果，当然是不对的。</p>

<p>那怎么办？很简单，给线性回归的结果套一个值域在0到1之间的函数不就行了呗。这种函数一般都跟<script type="math/tex">e</script>指数有关，所以我们有sigmod函数：</p>

<script type="math/tex; mode=display">\sigma(z) = \frac{1}{1+e^{-z}}</script>

<p>图像请看截图的左下角。</p>

<p>有了sigmod函数之后，只要令</p>

<script type="math/tex; mode=display">\hat{y} = \sigma(\boldsymbol{w}^\mathrm{T} \boldsymbol{x} + b)</script>

<p>就行了。加上了sigmod函数之后的回归就叫逻辑回归。有的方法把<script type="math/tex">b</script>也放进<script type="math/tex">\boldsymbol{w}</script>里面，不过这里不这么做。</p>

<p><img src="/img/in-post/post-DL-1/1-3.png" alt="" /></p>

<h4 id="代价损失函数">代价/损失函数</h4>

<p>代价/损失函数就是用来训练上面提到的<script type="math/tex">\boldsymbol{w}, b</script>的。这个函数越小，模型的效果越好（对于训练样本来说）。这里定义的损失函数只是对于一个训练样本来说的，而代价函数是对于整个训练样本集在某一堆参数下的评价。</p>

<p>学过线性拟合的同学们马上又该反应过来，一个典型的代价函数就是<script type="math/tex">L(\hat{y}, y) = (\hat{y} - y)^2</script>。但是这个函数的效果并不好，所以我们实际上用的是这个：</p>

<script type="math/tex; mode=display">L(\hat{y}, y) = -[y \ln{\hat{y}} + (1-y) \ln{(1-\hat{y})}]</script>

<p>大致地说为什么代价函数的样子是这样的原因是，差值的平方会导致函数非凸，但是上面的定义加上sigmod函数会使得<script type="math/tex">L</script>是个凸函数，没有局部极值。</p>

<p>对应的代价函数我们选择这个：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
J(\boldsymbol{w}, b) &= \frac{1}{m} \sum_{i=1}^{m} L(\hat{y}^{(i)}, y^{(i)}) \\
 &= - \frac{1}{m} \sum_{i=1}^{m} [y \ln{\hat{y}} + (1-y) \ln{(1-\hat{y})}]
\end{align} %]]></script>

<p>只要找到最小化<script type="math/tex">\boldsymbol{w}, b</script>的<script type="math/tex">J</script>，就行了。实际上逻辑回归就是一个小型的（可能还是最简单的）神经网络。</p>

<p><img src="/img/in-post/post-DL-1/1-4.png" alt="" /></p>

<h4 id="如何训练参数">如何训练参数？</h4>

<p>最小化<script type="math/tex">\boldsymbol{w}, b</script>就是在<script type="math/tex">J(\boldsymbol{w}, b)</script>这个表面上（简单来说）迭代地找到这个函数的最小值以及对应的参数。又该反应过来我们需要用到导数来指导我们向哪里走了。具体来说，是计算这个：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\boldsymbol{w} &:= \boldsymbol{w} - \alpha \frac{\partial J}{\partial \boldsymbol{w}}\\
b &:= b - \alpha \frac{\partial J}{\partial b}
\end{align} %]]></script>

<p><script type="math/tex">\alpha</script>被称为学习速率，是人选的；而后面的俩偏导数是需要计算的，在python中会被命名为<code class="highlighter-rouge">dw, db</code>。</p>

<p><img src="/img/in-post/post-DL-1/1-5.png" alt="" /></p>

<h4 id="正反向传递">正反向传递</h4>

<p>具体请看截图。正向传递的意思是从具体参数计算代价函数值的过程（蓝色、从左到右），而反向传递指的是计算导数的过程（红色、从右到左）。</p>

<p><img src="/img/in-post/post-DL-1/1-6.png" alt="" /></p>

<h4 id="导数的具体计算">导数的具体计算</h4>

<p>我们举一个简单的例子：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
z &= w_1 x_1 + w_2 x_2 + b \\
\hat{y} &= \sigma(z) \\
L(\hat{y}, y) &= -[y \ln{\hat{y}} + (1-y) \ln{(1-\hat{y})}] \\
\end{align} %]]></script>

<p>自然地</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\frac{dL}{d\hat{y}} &= -\frac{y}{\hat{y}} + \frac{1-y}{1-\hat{y}} \\
\frac{d\hat{y}}{dz} &= \hat{y} (1-\hat{y}) = \sigma (1-\sigma) = \hat{y} - y \\
\frac{dz}{dw_1} &= x_1 \frac{d\hat{y}}{dz} \\
\frac{dz}{dw_2} &= x_2 \frac{d\hat{y}}{dz} \\
\frac{dz}{db} &= \frac{d\hat{y}}{dz} \\
\end{align} %]]></script>

<p>在代码中我们基本上会用导数的分母作为变量名字。</p>

<p><img src="/img/in-post/post-DL-1/1-7.png" alt="" /></p>

<p>之后我们需要算的就是代价函数了。其实也没什么，就拿一个变量做代表吧：</p>

<script type="math/tex; mode=display">\frac{\partial J}{\partial w_1} = \frac{1}{m} \sum_{i=1}^{m} \frac{L^{(i)}}{\partial w_1}</script>

<p>具体的计算过程就是一个正向+反向传递的过程：</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 设置初值
</span><span class="n">J</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">dw1</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="o">...</span><span class="p">;</span> <span class="n">db</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">m</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
  <span class="c1"># Forward propagation
</span>  <span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span>
  <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">sigmod</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
  <span class="n">J</span> <span class="o">+=</span> <span class="n">L</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
  <span class="c1"># Backward propagation
</span>  <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
  <span class="n">dw1</span> <span class="o">+=</span> <span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
  <span class="o">...</span>
  <span class="n">db</span> <span class="o">+=</span> <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">J</span> <span class="o">/=</span> <span class="n">m</span><span class="p">;</span> <span class="n">dw1</span> <span class="o">/=</span> <span class="n">m</span><span class="p">;</span> <span class="o">...</span><span class="p">;</span> <span class="n">db</span> <span class="o">/=</span> <span class="n">m</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">w1</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dw1</span>
<span class="o">...</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">db</span>
</code></pre></div></div>

<p>然后继续迭代。所以“正常”情况下要用两个循环嵌套，显然是不现实的，所以要用到python的向量来做。</p>

<h4 id="向量化">向量化</h4>

<p>当<code class="highlighter-rouge">w</code>是一个列向量的时候，比如要算<code class="highlighter-rouge">z[i] = w.T * x[i] + b</code>的时候，可以这么做：<code class="highlighter-rouge">np.dot(w,x)+b</code>。高级一点的话直接和矩阵相乘：<code class="highlighter-rouge">np.dot(w,X)+b</code>。
<code class="highlighter-rouge">dz[i] = a[i] - y[i]</code>可以转为<code class="highlighter-rouge">dZ = A - Y</code>；<code class="highlighter-rouge">dw1 += x1[i]dz[i]</code> -&gt; <code class="highlighter-rouge">dW = 1/m * X*dZ.T</code>；<code class="highlighter-rouge">db += dz[i]</code> -&gt; <code class="highlighter-rouge">1/m * np.sum(dZ)</code>。还要注意的一点是在<code class="highlighter-rouge">reshape</code>的时候最好把行列都写全。</p>

<h3 id="浅神经网络">浅神经网络</h3>

<p>[1],[2]代表神经网络的层数；(i)代表某一个训练样本（都是上标）。下标不带任何括号，表示某一层中的第几个节点。当有两层的时候，刚开始的输入是<script type="math/tex">x</script>，但是之后的输入会变成了<script type="math/tex">a^{[i]}</script>这样。</p>

<h4 id="神经网络的命名">神经网络的命名</h4>

<p>输入层、隐层、输出层。<script type="math/tex">a^{[0]} = x</script>。在数层数的时候，不算输入层。每一层都有一套<script type="math/tex">w, b</script>配套。</p>

<p><img src="/img/in-post/post-DL-1/2-1.png" alt="" /></p>

<p>每一个节点都有两步：线性回归以及丢进sigmod函数里面。所以都有两条公式：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
  z_1^{[1]} &= w_1^{[1]T}x + b_1^{[1]}, a_1^{[1]} = \sigma(z_1^{[1]}) \\
  z_2^{[1]} &= w_2^{[1]T}x + b_2^{[1]}, a_2^{[1]} = \sigma(z_2^{[1]}) \\
  z_3^{[1]} &= w_3^{[1]T}x + b_3^{[1]}, a_3^{[1]} = \sigma(z_3^{[1]}) \\
  z_4^{[1]} &= w_4^{[1]T}x + b_4^{[1]}, a_4^{[1]} = \sigma(z_4^{[1]}) \\
  \vdots
\end{align} %]]></script>

<p>能看到这些式子排得很整齐，那何不矩阵化了呢？</p>

<script type="math/tex; mode=display">z^{[1]} =
  \begin{bmatrix}
    z_1^{[1]} \\ z_2^{[1]} \\ z_3^{[1]} \\ z_4^{[1]}
  \end{bmatrix}
  =
  \begin{bmatrix}
    \cdots w_1^{[1]T} \cdots \\ \cdots w_2^{[1]T} \cdots \\ \cdots w_3^{[1]T} \cdots \\ \cdots w_4^{[1]T} \cdots
  \end{bmatrix}
  \begin{bmatrix}
    x_1 \\ x_2 \\ x_3
  \end{bmatrix}
  +
  \begin{bmatrix}
    b_1^{[1]} \\ b_2^{[1]} \\ b_3^{[1]} \\ b_4^{[1]}
  \end{bmatrix}
  = W^{[1]} x + b^{[1]}</script>

<p><script type="math/tex">W^{[i]}</script>是一个<script type="math/tex">n^{[i]}</script>（本层节点数）行<script type="math/tex">n^{[i-1]}</script>（上层节点数）列的矩阵，也就是负责将上层节点的结果转成本层节点的结果。<script type="math/tex">b^{[i]}</script>自然是<script type="math/tex">n^{[i]}</script>行1列的向量了。最后再将结果各自丢进sigmod函数里面，不说了。</p>

<p>如果写出个通式的话也行：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
   z^{[i]} &=  W^{[i]} a^{[i-1]} + b^{[i]} \\
   a^{[i]} &= \sigma(z^{[i]})
\end{align} %]]></script>

<p>不过注意<script type="math/tex">a^{[0]}</script>就是输入值，没有放入sigmod函数里面。同时矩阵<script type="math/tex">W^{[i]}</script>是一个“行矩阵”，每一行是每一个节点的系数的转置<script type="math/tex">[\cdots w_i^{[1]T} \cdots]</script>。</p>

<p><img src="/img/in-post/post-DL-1/2-2.png" alt="" /></p>

<p><img src="/img/in-post/post-DL-1/2-3.png" alt="" /></p>

<h4 id="对于整个训练样本">对于整个训练样本</h4>

<p>当然可以继续矩阵化啦。把整个训练样本的列向量<script type="math/tex">x,b</script>排成一行（这也是惟一的排法了），结果<script type="math/tex">z</script>也这么做，然后大写，就变成了：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
Z^{[1]} &=
  \begin{bmatrix}
    \vdots & \vdots &  & \vdots \\
    z^{[1](1)} & z^{[1](2)} & \cdots & z^{[1](m)} \\
    \vdots & \vdots &  & \vdots \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    \cdots w_1^{[1]T} \cdots \\ \cdots w_2^{[1]T} \cdots \\ \vdots \\ \cdots w_{[i]}^{[1]T} \cdots
  \end{bmatrix}
  \begin{bmatrix}
    \vdots & \vdots &  & \vdots \\
    x^{(1)} & x^{(2)} & \cdots & x^{(m)} \\
    \vdots & \vdots &  & \vdots \\
  \end{bmatrix}
  +
  \begin{bmatrix}
    \vdots \\ b^{[1]} \\ \vdots \\
  \end{bmatrix} \\
  &= W^{[1]} X + b^{[1]}
\end{align} %]]></script>

<p>或者对于某一层来说</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
Z^{[i]} &=
  \begin{bmatrix}
    \vdots & \vdots &  & \vdots \\
    z^{[i](1)} & z^{[i](2)} & \cdots & z^{[i](m)} \\
    \vdots & \vdots &  & \vdots \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    \cdots w_1^{[i]T} \cdots \\ \cdots w_2^{[i]T} \cdots \\ \vdots \\ \cdots w_{[i]}^{[i]T} \cdots
  \end{bmatrix}
  \begin{bmatrix}
    \vdots & \vdots &  & \vdots \\
    a^{[i](1)} & a^{[i](2)} & \cdots & a^{[i](m)} \\
    \vdots & \vdots &  & \vdots \\
  \end{bmatrix}
  +
  \begin{bmatrix}
    \vdots \\ b^{[i]} \\ \vdots \\
  \end{bmatrix} \\
  &= W^{[i]} X + b^{[i]}
\end{align} %]]></script>

<p>总结一下，<script type="math/tex">Z^{[i]}</script>是一个<script type="math/tex">n^{[i]}</script>行<script type="math/tex">m</script>列的矩阵，<script type="math/tex">W^{[i]}</script>是一个<script type="math/tex">n^{[i]}</script>行<script type="math/tex">n^{[i-1]}</script>列的矩阵，<script type="math/tex">A^{[i]}</script>是一个<script type="math/tex">n^{[i]}</script>行<script type="math/tex">m</script>列的矩阵，<script type="math/tex">b^{[i]}</script>本来应该也是一个<script type="math/tex">[i]</script>行<script type="math/tex">m</script>列的矩阵（每列都是一样），但是在python里面，同行数的向量和矩阵相加时，向量会被自动展成相应的矩阵，所以也就不用写成大写的<script type="math/tex">B^{[i]}</script>了。可以留意的是向量/矩阵的偏导数和它自己有相同的形状。</p>

<p>通式：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
   Z^{[i]} &=  W^{[i]} A^{[i-1]} + b^{[i]} \\
   A^{[i]} &= \sigma(Z^{[i]})
\end{align} %]]></script>

<p><img src="/img/in-post/post-DL-1/2-4.png" alt="" /></p>

<p><img src="/img/in-post/post-DL-1/2-5.png" alt="" /></p>

<p><img src="/img/in-post/post-DL-1/2-6.png" alt="" /></p>

<h4 id="不同的activation-function">不同的activation function</h4>

<p>sigmod函数的问题是它恒为正，所以算出来的结果不会关于0对称。所以隐藏层一般用<script type="math/tex">\tanh(z)</script>而不是sigmod，但是输出层还是用回sigmod。</p>

<p>但是<script type="math/tex">\tanh(z)</script>也还是有问题：当<script type="math/tex">z</script>很大的时候，斜率很小，所以会降低学习速率；所以也可以用RelU函数（折线），或者将RelU小于0的折线再改小一点。即使斜率为0实际上也还是可以的。</p>

<p><img src="/img/in-post/post-DL-1/2-7.png" alt="" /></p>

<h4 id="为什么activation-function不是线性的">为什么activation function不是线性的？</h4>

<p>也就是说为什么要给它套上一个奇怪的函数？</p>

<p>线性的AF意味着从头到脚都是线性的，也就没有意义（人又不是线性思考的233）。例外是在回归里面，当输出不在0到1之间的时候，最后一层可以用线性的。但是其他层还是要用非线性的。</p>

<p><img src="/img/in-post/post-DL-1/2-8.png" alt="" /></p>

<h4 id="af的导数">AF的导数</h4>

<p>求就是了，又不是不会。按照链式反应（chain rule啦）来求就行了。按照之前的结论其实<script type="math/tex">\frac{dL}{da^{[2]}}</script>不用求，有<script type="math/tex">\frac{dL}{dz^{[2]}} = a^{[2]} - y</script>。</p>

<p>完全向量化之后出现的<script type="math/tex">\frac{1}{m}</script>以及求和是因为之前对某一个训练样本来说，只能计算损失函数；但是对于整个训练样本集来说，我们计算的是代价函数，而<script type="math/tex">J = \frac{1}{m}\sum L</script>，所以多了点东西。</p>

<p><img src="/img/in-post/post-DL-1/2-9.png" alt="" /></p>

<p><img src="/img/in-post/post-DL-1/2-10.png" alt="" /></p>

<h4 id="随机初值">随机初值</h4>

<p>参数全为0的话，所有的节点都是一样（对称）的，就没用了。所以要随机取值。一般会将<script type="math/tex">w</script>取为很小的高斯随机数，但是<script type="math/tex">b</script>没所谓。很小的高斯随机数是为了取到比较大的导数值，防止落到导数很靠近0的地方。</p>

<p><img src="/img/in-post/post-DL-1/2-11.png" alt="" /></p>

<h3 id="深神经网络">深神经网络</h3>

<p>多层的神经网络和2层的其实是一样的，只不过多了一堆循环而已。</p>

<h4 id="为什么要用深的神经网络">为什么要用深的神经网络？</h4>

<p>层数从小到大：简单元素 -&gt; 复杂元素。所以我们需要比较多层，但是每层里面的节点不一定多。</p>

<p>比单层的好用</p>

<p><img src="/img/in-post/post-DL-1/3-1.png" alt="" /></p>

<h4 id="正向传递">正向传递</h4>

<p>计算不同层数的时候可以用循环。</p>

<p><img src="/img/in-post/post-DL-1/3-2.png" alt="" /></p>

<h4 id="反向传递">反向传递</h4>

<p><img src="/img/in-post/post-DL-1/3-3.png" alt="" /></p>

<h4 id="具体做法">具体做法</h4>

<p><img src="/img/in-post/post-DL-1/3-4.png" alt="" /></p>

<p>因为反向传递的时候有一个<script type="math/tex">g'^{[l]}(z^{[l]})</script>，所以在正向传递的时候要将<script type="math/tex">z^{[l]}</script>记录下来。</p>

<h4 id="参数和超参数">参数和超参数</h4>

<p>超参数：人为指定的参数</p>

<p>看超参数和<script type="math/tex">J</script>的关系（选多少的时候最小），是一个经验方法。最佳的超参数集可能会随着时间变化（电脑更新了）</p>

<p>这次的笔记本还演示了如何载入另外一个笔记本的函数。</p>


                <hr style="visibility: hidden;">

                


                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2018/03/08/OASP9/" >
                        Previous<br>
                        <span>OASP笔记 - 9</span>
                        </a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2018/03/14/DL-2/" >
                        Next<br>
                        <span>Deep Learning笔记2</span>
                        </a>
                    </li>
                    
                </ul>


                

                
                <!-- disqus 评论框 start -->
                <div class="comment">
                    <div id="disqus_thread" class="disqus-thread"></div>
                </div>
                <!-- disqus 评论框 end -->
                

                
            </div>

    <!-- Side Catalog Container -->
        

    <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <hr class="hidden-sm hidden-xs">
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
        				
                            
                				<a href="/tags/#random" title="random" rel="4">
                                    random
                                </a>
                            
        				
                            
                				<a href="/tags/#hanging-around" title="hanging-around" rel="6">
                                    hanging-around
                                </a>
                            
        				
                            
                				<a href="/tags/#astro-ph" title="astro-ph" rel="4">
                                    astro-ph
                                </a>
                            
        				
                            
                				<a href="/tags/#experience" title="experience" rel="2">
                                    experience
                                </a>
                            
        				
                            
                				<a href="/tags/#learning" title="learning" rel="37">
                                    learning
                                </a>
                            
        				
                            
        				
        			</div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">
                    
                        <li><a href="http://gerry.lamost.org/blog/">Gerry</a></li>
                    
                        <li><a href="http://fmajor.lamost.org/blog/">Fmajor's Blog</a></li>
                    
                        <li><a href="http://huangxuan.me">Hux Blog</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>








<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = "mingjiejian-github-io";
    var disqus_identifier = "mingjiejian-github-io//2018/03/14/DL-1/";
    var disqus_url = "http://localhost:4000/2018/03/14/DL-1/";

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<!-- disqus 公共JS代码 end -->




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'always',
          placement: 'right',
          icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    
                    <li>
                        <a href="/feed.xml">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    

                    <!-- add Weibo, Zhihu by Hux, add target = "_blank" to <a> by Hux -->
                    
                    <li>
                        <a target="_blank" href="https://www.zhihu.com/people/jian-ming-jie">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa  fa-stack-1x fa-inverse">知</i>
                            </span>
                        </a>
                    </li>
                    
                    


                    
                    
                    <li>
                        <a target="_blank" href="https://github.com/mingjieJian">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Mingjie's Blog 2019
                    <br>
                    Theme by <a href="http://huangxuan.me">Hux</a> |
                    <iframe
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="91px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=huxpro&repo=huxpro.github.io&type=star&count=true" >
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js "></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- 
     Because of the native support for backtick-style fenced code blocks 
     right within the Markdown is landed in Github Pages, 
     From V1.6, There is no need for Highlight.js, 
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0  
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/    
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->

<script>
    // dynamic User by Hux
    var _gaId = 'UA-100940889-1';
    var _gaDomain = 'mingjiejian.github.io';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>



<!-- Baidu Tongji -->




<!-- Image to hack wechat -->
<img src="/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->


</body>

<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML'></script>

</html>
